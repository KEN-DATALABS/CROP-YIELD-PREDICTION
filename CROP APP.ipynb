{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fb5dc-2213-474b-abef-70ae0006c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Set up Streamlit layout to use full screen width\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "# App Title\n",
    "st.title(\"CROP YIELD PREDICTION APP\")\n",
    "\n",
    "# Introductory markdown explaining the app's purpose\n",
    "st.markdown(\"\"\"\n",
    "Welcome to the Crop Yield Prediction App!  \n",
    "This tool guides you from dataset upload to model prediction through interactive analysis, cleaning, and visualization.  \n",
    "Upload your dataset or use a sample to begin your journey!\n",
    "\"\"\")\n",
    "\n",
    "# Sidebar navigation options for different stages of the app\n",
    "st.sidebar.title(\"Navigation\")\n",
    "options = st.sidebar.radio(\"Select Step:\", \n",
    "                          [\"Upload Data\", \"Data Cleaning\", \"EDA\", \n",
    "                           \"Visualization\", \"Prediction\", \"Insights\"])\n",
    "\n",
    "# Initialize Streamlit session state variables to persist data across steps\n",
    "for key in ['df', 'cleaned_df', 'target', 'model_type', 'model', 'report']:\n",
    "    if key not in st.session_state:\n",
    "        st.session_state[key] = None\n",
    "\n",
    "# Define the required columns expected in the uploaded dataset\n",
    "expected_columns = [\n",
    "    \"Area\", \n",
    "    \"Item\", \n",
    "    \"Year\", \n",
    "    \"hg/ha_yield\", \n",
    "    \"average_rain_fall_mm_per_year\", \n",
    "    \"pesticides_tonnes\", \n",
    "    \"avg_temp\"\n",
    "]\n",
    "\n",
    "# Step 1: Upload or Load Dataset\n",
    "if options == \"Upload Data\":\n",
    "    st.header(\"Upload or Select Dataset\")\n",
    "\n",
    "    df = None  # Initialize an empty DataFrame variable to avoid reference issues\n",
    "\n",
    "    # Provide user with two options: upload own dataset or use a default one\n",
    "    data_source = st.radio(\"Choose a data source:\", ['Upload your dataset', 'Use default dataset'])\n",
    "\n",
    "    # CASE 1: Uploading a custom dataset\n",
    "    if data_source == 'Upload your dataset':\n",
    "        uploaded_file = st.file_uploader(\"Upload CSV or Excel file\", type=[\"csv\", \"xlsx\"])\n",
    "\n",
    "        # Process uploaded file if provided\n",
    "        if uploaded_file:\n",
    "            try:\n",
    "                # Read based on file type\n",
    "                if uploaded_file.name.endswith(\".csv\"):\n",
    "                    df = pd.read_csv(uploaded_file)\n",
    "                else:\n",
    "                    df = pd.read_excel(uploaded_file)\n",
    "\n",
    "                st.success(\"File uploaded and read successfully.\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error reading file: {e}\")\n",
    "                df = None  # Prevent later usage if file fails\n",
    "\n",
    "    # CASE 2: Load default dataset from project directory\n",
    "    else:\n",
    "        try:\n",
    "            df = pd.read_csv(\"yield_df.csv\")  # Ensure this file is available in the working folder\n",
    "            st.success(\"Default dataset loaded successfully.\")\n",
    "        except FileNotFoundError:\n",
    "            st.error(\"Default dataset not found in the directory.\")\n",
    "            df = None\n",
    "\n",
    "        # Provide sample structure for download to guide new users\n",
    "        st.markdown(\"Don't have a dataset? [Download Example CSV](https://raw.githubusercontent.com/datasciencedojo/datasets/master/Agricultural%20Production.csv)\")\n",
    "        \n",
    "        # Provide downloadable blank template CSV with only column headers\n",
    "        st.download_button(\n",
    "            label=\"Download Example Dataset\",\n",
    "            data=pd.DataFrame(columns=expected_columns).to_csv(index=False),\n",
    "            file_name='example_crop_data.csv',\n",
    "            mime='text/csv'\n",
    "        )\n",
    "\n",
    "    # Proceed with data inspection if a dataset is successfully loaded\n",
    "    if df is not None:\n",
    "        # Check for missing expected columns in uploaded/default data\n",
    "        missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "\n",
    "        if missing_columns:\n",
    "            # Display error if required columns are missing\n",
    "            st.error(\"Dataset is missing the following required columns:\")\n",
    "            st.write(missing_columns)\n",
    "        else:\n",
    "            # Check and drop any extra columns not needed\n",
    "            extra_columns = [col for col in df.columns if col not in expected_columns]\n",
    "            if extra_columns:\n",
    "                df = df[expected_columns]  # Keep only necessary columns\n",
    "                st.warning(f\"Extra columns dropped: {extra_columns}\")\n",
    "\n",
    "            # Store valid DataFrame in session state for reuse in other steps\n",
    "            st.session_state.df = df\n",
    "\n",
    "            # Display feedback and data summaries\n",
    "            st.success(\"Dataset is valid and ready for analysis!\")\n",
    "\n",
    "            # Data preview (first 5 rows)\n",
    "            st.subheader(\"Data Preview\")\n",
    "            st.dataframe(df.head())\n",
    "\n",
    "            # Show data types of all columns\n",
    "            st.subheader(\"Data Types\")\n",
    "            st.dataframe(pd.DataFrame(df.dtypes, columns=[\"Data Type\"]))\n",
    "\n",
    "            # Basic info: number of rows and columns\n",
    "            st.subheader(\"Dataset Overview\")\n",
    "            st.write(f\"Rows: {df.shape[0]} | Columns: {df.shape[1]}\")\n",
    "\n",
    "            # Descriptive statistics for all columns\n",
    "            st.subheader(\"Descriptive Statistics\")\n",
    "            st.dataframe(df.describe(include='all'))\n",
    "\n",
    "            # Missing values summary\n",
    "            st.subheader(\"Missing Values\")\n",
    "            missing_df = pd.DataFrame(df.isna().sum(), columns=['Missing Values'])\n",
    "            missing_df[\"Percentage\"] = (missing_df['Missing Values'] / len(df)) * 100\n",
    "            st.dataframe(missing_df)\n",
    "\n",
    "            # Count and show number of duplicate rows\n",
    "            st.subheader(\"Duplicate Rows\")\n",
    "            st.write(f\"Number of duplicate rows: {df.duplicated().sum()}\")\n",
    "            \n",
    "# Step 2: Data Cleaning\n",
    "elif options == \"Data Cleaning\":\n",
    "    st.header(\"Data Cleaning\")\n",
    "\n",
    "    # Ensure a dataset is loaded before proceeding\n",
    "    if st.session_state.df is not None:\n",
    "        df = st.session_state.df\n",
    "\n",
    "        st.subheader(\"Data Issues Detected\")\n",
    "\n",
    "        issues = []  # List to hold all detected data quality issues\n",
    "\n",
    "        # 1. Check for Missing Values\n",
    "        total_missing = df.isna().sum().sum()\n",
    "        if total_missing > 0:\n",
    "            issues.append(f\"Missing values detected: {total_missing} total\")\n",
    "\n",
    "        # 2. Check for Duplicate Rows\n",
    "        duplicates = df.duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            issues.append(f\"Duplicate rows detected: {duplicates}\")\n",
    "\n",
    "        # 3. Outlier Detection using IQR method\n",
    "        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        outlier_cols = []  # Keep track of columns that contain outliers\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "            if not outliers.empty:\n",
    "                issues.append(f\"Potential outliers detected in '{col}'\")\n",
    "                outlier_cols.append(col)\n",
    "\n",
    "        # Visualize detected outliers using boxplots\n",
    "        if outlier_cols:\n",
    "            st.write(\"The following columns have potential outliers:\")\n",
    "            for col in outlier_cols:\n",
    "                fig, ax = plt.subplots(figsize=(5, 3))\n",
    "                sns.boxplot(df[col], color='skyblue', ax=ax)\n",
    "                ax.set_title(f'Outliers in {col}', fontsize=12)\n",
    "                st.pyplot(fig)\n",
    "\n",
    "        # 4. Check for numeric data stored as text\n",
    "        for col in df.select_dtypes(include='object').columns:\n",
    "            try:\n",
    "                pd.to_numeric(df[col])  # Attempt conversion\n",
    "                issues.append(f\"Column '{col}' contains numeric data stored as text\")\n",
    "            except:\n",
    "                pass  # If conversion fails, ignore\n",
    "\n",
    "        # Display detected issues\n",
    "        if issues:\n",
    "            st.warning(\"The following data issues were found:\")\n",
    "            for issue in issues:\n",
    "                st.markdown(f\"- {issue}\")\n",
    "        else:\n",
    "            st.success(\"No major data issues detected!\")\n",
    "\n",
    "\n",
    "        # Data Cleaning Interface\n",
    "        st.subheader(\"Data Cleaning Options\")\n",
    "\n",
    "        # Let user choose multiple cleaning actions\n",
    "        cleaning_options = st.multiselect(\n",
    "            \"Select cleaning actions to apply:\",\n",
    "            [\n",
    "                \"Remove duplicate rows\",\n",
    "                \"Fill missing values (numeric)\",\n",
    "                \"Fill missing values (categorical)\",\n",
    "                \"Remove rows with missing values\",\n",
    "                \"Remove columns with high missing values (>30%)\",\n",
    "                \"Convert text to numeric where possible\",\n",
    "                \"Remove outliers (for numeric columns)\",\n",
    "                \"Standardize column names\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Button to apply selected cleaning steps\n",
    "        if st.button(\"Clean Data\"):\n",
    "            cleaned_df = df.copy()  # Work on a copy of the dataset\n",
    "\n",
    "            # Remove duplicates\n",
    "            if \"Remove duplicate rows\" in cleaning_options:\n",
    "                cleaned_df = cleaned_df.drop_duplicates()\n",
    "\n",
    "            # Fill missing numeric values with column mean\n",
    "            if \"Fill missing values (numeric)\" in cleaning_options:\n",
    "                from sklearn.impute import SimpleImputer\n",
    "                numeric_cols = cleaned_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "                imputer = SimpleImputer(strategy='mean')\n",
    "                cleaned_df[numeric_cols] = imputer.fit_transform(cleaned_df[numeric_cols])\n",
    "\n",
    "            # Fill missing categorical values with mode\n",
    "            if \"Fill missing values (categorical)\" in cleaning_options:\n",
    "                cat_cols = cleaned_df.select_dtypes(include=['object']).columns\n",
    "                for col in cat_cols:\n",
    "                    if cleaned_df[col].isnull().sum() > 0:\n",
    "                        cleaned_df[col].fillna(cleaned_df[col].mode()[0], inplace=True)\n",
    "\n",
    "            # Drop any row that still contains missing values\n",
    "            if \"Remove rows with missing values\" in cleaning_options:\n",
    "                cleaned_df.dropna(inplace=True)\n",
    "\n",
    "            # Drop columns where more than 30% of values are missing\n",
    "            if \"Remove columns with high missing values (>30%)\" in cleaning_options:\n",
    "                threshold = len(cleaned_df) * 0.3\n",
    "                cleaned_df.dropna(axis=1, thresh=threshold, inplace=True)\n",
    "\n",
    "            # Convert any column with numeric text to actual numeric type\n",
    "            if \"Convert text to numeric where possible\" in cleaning_options:\n",
    "                for col in cleaned_df.columns:\n",
    "                    if cleaned_df[col].dtype == 'object':\n",
    "                        try:\n",
    "                            cleaned_df[col] = pd.to_numeric(cleaned_df[col])\n",
    "                        except:\n",
    "                            pass  # Ignore conversion errors\n",
    "\n",
    "            # Remove outliers using the 3-standard-deviation rule\n",
    "            if \"Remove outliers (for numeric columns)\" in cleaning_options:\n",
    "                for col in cleaned_df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "                    mean = cleaned_df[col].mean()\n",
    "                    std = cleaned_df[col].std()\n",
    "                    cleaned_df = cleaned_df[\n",
    "                        (cleaned_df[col] <= mean + 3 * std) &\n",
    "                        (cleaned_df[col] >= mean - 3 * std)\n",
    "                    ]\n",
    "\n",
    "            # Rename all columns to lowercase with underscores (standard format)\n",
    "            if \"Standardize column names\" in cleaning_options:\n",
    "                cleaned_df.columns = cleaned_df.columns.str.lower().str.strip().str.replace(\" \", \"_\")\n",
    "\n",
    "            # Save cleaned data to session state for reuse in later steps\n",
    "            st.session_state.cleaned_df = cleaned_df\n",
    "\n",
    "            # Display cleaning result\n",
    "            st.success(\"Data cleaning completed successfully!\")\n",
    "\n",
    "            # Preview cleaned dataset\n",
    "            st.subheader(\"Cleaned Data Preview\")\n",
    "            st.dataframe(cleaned_df.head())\n",
    "\n",
    "            # Cleaning summary statistics\n",
    "            st.subheader(\"Cleaning Summary\")\n",
    "            st.write(f\"Original shape: {df.shape}\")\n",
    "            st.write(f\"New shape: {cleaned_df.shape}\")\n",
    "            st.write(f\"Rows removed: {df.shape[0] - cleaned_df.shape[0]}\")\n",
    "            st.write(f\"Columns removed: {df.shape[1] - cleaned_df.shape[1]}\")\n",
    "\n",
    "    else:\n",
    "        # Message shown if user tries to access this step before uploading data\n",
    "        st.warning(\"Please upload a dataset first in the 'Upload Data' section.\")\n",
    "\n",
    "\n",
    "# # STEP 3: Data Visualization\n",
    "# elif options == \"Data Visualization\":\n",
    "#     st.header(\"Data Visualization\")\n",
    "\n",
    "#     if st.session_state.cleaned_df is not None:\n",
    "#         df = st.session_state.cleaned_df\n",
    "        \n",
    "# st.subheader(\"üìà Exploratory Data Analysis\")\n",
    "# st.write(\"Select columns to visualize\")\n",
    "\n",
    "# numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "# selected_col = st.selectbox(\"Choose a numeric column\", numeric_cols)\n",
    "\n",
    "# if selected_col:\n",
    "#     fig, ax = plt.subplots()\n",
    "#     sns.histplot(df[selected_col], kde=True, ax=ax)\n",
    "#     st.pyplot(fig)\n",
    "\n",
    "# # STEP 4: Preprocessing\n",
    "# st.subheader(\"‚öôÔ∏è Data Preprocessing\")\n",
    "\n",
    "# target_col = st.selectbox(\"üéØ Select Target Column\", df.columns)\n",
    "# X = df.drop(columns=[target_col])\n",
    "# y = df[target_col]\n",
    "\n",
    "# # Encode categorical features\n",
    "# for col in X.select_dtypes(include=\"object\").columns:\n",
    "#     le = LabelEncoder()\n",
    "#     X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# # Encode target if needed\n",
    "# if y.dtype == 'object':\n",
    "#     y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# # Scaling features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# st.success(\"‚úÖ Preprocessing complete!\")\n",
    "\n",
    "# # STEP 5: Model Training\n",
    "# st.subheader(\"ü§ñ Model Development\")\n",
    "\n",
    "# test_size = st.slider(\"Select test size\", 0.1, 0.5, 0.2)\n",
    "# random_state = st.number_input(\"Random state (for reproducibility)\", value=42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=int(random_state))\n",
    "\n",
    "# model = RandomForestClassifier()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # STEP 6: Evaluation\n",
    "# st.subheader(\"üìã Model Evaluation\")\n",
    "# st.write(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# st.text(\"Classification Report:\")\n",
    "# st.text(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Predict with user input\n",
    "# st.subheader(\"üìù Make a Prediction\")\n",
    "# input_data = {}\n",
    "# for col in df.drop(columns=[target_col]).columns:\n",
    "#     value = st.text_input(f\"Enter value for {col}\")\n",
    "#     input_data[col] = value\n",
    "\n",
    "# if st.button(\"Predict\"):\n",
    "#     input_df = pd.DataFrame([input_data])\n",
    "\n",
    "#     for col in input_df.columns:\n",
    "#         if input_df[col].dtype == 'object':\n",
    "#             input_df[col] = LabelEncoder().fit(df[col]).transform(input_df[col])\n",
    "\n",
    "#     input_df_scaled = scaler.transform(input_df)\n",
    "#     prediction = model.predict(input_df_scaled)\n",
    "#     st.success(f\"üéâ Predicted class: {prediction[0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
